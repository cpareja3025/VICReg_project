{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb27909faf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True)\n",
    "image, label = train_dataset[11]\n",
    "print(label)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "a = transforms.Resize(252)(image)\n",
    "plt.imshow(a, cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Vanilla CNN for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1875], Loss: 0.6733 \n",
      "Epoch [1/1], Step [200/1875], Loss: 0.2787 \n",
      "Epoch [1/1], Step [300/1875], Loss: 0.3791 \n",
      "Epoch [1/1], Step [400/1875], Loss: 0.3074 \n",
      "Epoch [1/1], Step [500/1875], Loss: 0.1071 \n",
      "Epoch [1/1], Step [600/1875], Loss: 0.1207 \n",
      "Epoch [1/1], Step [700/1875], Loss: 0.1889 \n",
      "Epoch [1/1], Step [800/1875], Loss: 0.4430 \n",
      "Epoch [1/1], Step [900/1875], Loss: 0.2902 \n",
      "Epoch [1/1], Step [1000/1875], Loss: 0.1656 \n",
      "Epoch [1/1], Step [1100/1875], Loss: 0.0938 \n",
      "Epoch [1/1], Step [1200/1875], Loss: 0.2238 \n",
      "Epoch [1/1], Step [1300/1875], Loss: 0.0591 \n",
      "Epoch [1/1], Step [1400/1875], Loss: 0.0747 \n",
      "Epoch [1/1], Step [1500/1875], Loss: 0.1415 \n",
      "Epoch [1/1], Step [1600/1875], Loss: 0.0189 \n",
      "Epoch [1/1], Step [1700/1875], Loss: 0.0102 \n",
      "Epoch [1/1], Step [1800/1875], Loss: 0.0465 \n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.1307, std=0.3081)])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(1014, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, 1014)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 20, 20])\n",
      "torch.Size([64, 12, 20, 20])\n",
      "torch.Size([64, 6, 18, 18])\n",
      "torch.Size([64, 6, 9, 9])\n",
      "torch.Size([64, 256])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def data_aug(img_tensor):\n",
    "    aug = transforms.RandomResizedCrop(20, scale=(0.08,0.1))(img_tensor)\n",
    "    aug = transforms.RandomHorizontalFlip(p=0.5)(aug)\n",
    "    aug = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)(aug)\n",
    "    #aug = transforms.RandomGrayscale(0.2)(aug)\n",
    "    aug = transforms.GaussianBlur(kernel_size=23, sigma=0.5)(aug)\n",
    "    aug = transforms.RandomSolarize(threshold=0.3,p=0.1)(aug)\n",
    "    #aug = transforms.Normalize()(aug)\n",
    "    return aug\n",
    "    \n",
    "\n",
    "N = 64\n",
    "data = torch.rand(N,1,20,20)\n",
    "data = data_aug(data)\n",
    "print(data.shape)\n",
    "conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "max_pool = nn.MaxPool2d(2, 2)\n",
    "x = conv1(data)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = max_pool(x)\n",
    "print(x.shape)\n",
    "x = x.view(-1,486)\n",
    "fc1 = nn.Linear(486, 256)\n",
    "x = fc1(x)\n",
    "print(x.shape)\n",
    "fc2 = nn.Linear(256, 128)\n",
    "x = fc2(x)\n",
    "print(x.shape)\n",
    "fc3 = nn.Linear(128,10)\n",
    "x = fc3(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Torch Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1875], Loss: 2.3179 \n",
      "Epoch [1/1], Step [200/1875], Loss: 2.3040 \n",
      "Epoch [1/1], Step [300/1875], Loss: 2.3448 \n",
      "Epoch [1/1], Step [400/1875], Loss: 2.2355 \n",
      "Epoch [1/1], Step [500/1875], Loss: 2.2036 \n",
      "Epoch [1/1], Step [600/1875], Loss: 2.2517 \n",
      "Epoch [1/1], Step [700/1875], Loss: 2.2996 \n",
      "Epoch [1/1], Step [800/1875], Loss: 2.2830 \n",
      "Epoch [1/1], Step [900/1875], Loss: 2.2156 \n",
      "Epoch [1/1], Step [1000/1875], Loss: 2.2059 \n",
      "Epoch [1/1], Step [1100/1875], Loss: 2.3068 \n",
      "Epoch [1/1], Step [1200/1875], Loss: 2.2305 \n",
      "Epoch [1/1], Step [1300/1875], Loss: 2.3327 \n",
      "Epoch [1/1], Step [1400/1875], Loss: 2.2938 \n",
      "Epoch [1/1], Step [1500/1875], Loss: 2.0780 \n",
      "Epoch [1/1], Step [1600/1875], Loss: 2.1877 \n",
      "Epoch [1/1], Step [1700/1875], Loss: 2.1863 \n",
      "Epoch [1/1], Step [1800/1875], Loss: 2.1506 \n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.1307, std=0.3081)])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class CNN_augs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_augs, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(486, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # change output channels to multiples of 2 --> maybe 16, 32??\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1,486)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def data_aug(self, img_tensor):\n",
    "        aug = transforms.RandomResizedCrop(20, scale=(0.08,0.1))(img_tensor)\n",
    "        aug = transforms.RandomHorizontalFlip(p=0.5)(aug)\n",
    "        aug = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)(aug)\n",
    "        #aug = transforms.RandomGrayscale(0.2)(aug)\n",
    "        aug = transforms.GaussianBlur(kernel_size=23, sigma=0.5)(aug)\n",
    "        aug = transforms.RandomSolarize(threshold=0.3,p=0.1)(aug)\n",
    "        #aug = transforms.Normalize()(aug)\n",
    "        return aug\n",
    "    \n",
    "\n",
    "model = CNN_augs().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        images = model.data_aug(images)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        # compute loss between two different data augs!\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_augs                                 [128, 10]                 --\n",
       "├─Conv2d: 1-1                            [128, 12, 20, 20]         24\n",
       "├─Conv2d: 1-2                            [128, 6, 18, 18]          654\n",
       "├─MaxPool2d: 1-3                         [128, 6, 9, 9]            --\n",
       "├─Linear: 1-4                            [128, 256]                124,672\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 10]                 1,290\n",
       "==========================================================================================\n",
       "Total params: 159,536\n",
       "Trainable params: 159,536\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 48.69\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 7.31\n",
       "Params size (MB): 0.64\n",
       "Estimated Total Size (MB): 8.15\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_augs().to(device=device)\n",
    "summary(model, input_size=[(batch_size, 1, 20, 20)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing VICReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.1966 \n",
      "Epoch [1/5], Step [200/938], Loss: 0.1955 \n",
      "Epoch [1/5], Step [300/938], Loss: 0.1955 \n",
      "Epoch [1/5], Step [400/938], Loss: 0.1956 \n",
      "Epoch [1/5], Step [500/938], Loss: 0.1955 \n",
      "Epoch [1/5], Step [600/938], Loss: 0.1959 \n",
      "Epoch [1/5], Step [700/938], Loss: 0.1942 \n",
      "Epoch [1/5], Step [800/938], Loss: 0.1950 \n",
      "Epoch [1/5], Step [900/938], Loss: 0.1955 \n",
      "Epoch [2/5], Step [100/938], Loss: 0.1957 \n",
      "Epoch [2/5], Step [200/938], Loss: 0.1956 \n",
      "Epoch [2/5], Step [300/938], Loss: 0.1950 \n",
      "Epoch [2/5], Step [400/938], Loss: 0.1949 \n",
      "Epoch [2/5], Step [500/938], Loss: 0.1941 \n",
      "Epoch [2/5], Step [600/938], Loss: 0.1950 \n",
      "Epoch [2/5], Step [700/938], Loss: 0.1957 \n",
      "Epoch [2/5], Step [800/938], Loss: 0.1947 \n",
      "Epoch [2/5], Step [900/938], Loss: 0.1934 \n",
      "Epoch [3/5], Step [100/938], Loss: 0.1948 \n",
      "Epoch [3/5], Step [200/938], Loss: 0.1964 \n",
      "Epoch [3/5], Step [300/938], Loss: 0.1950 \n",
      "Epoch [3/5], Step [400/938], Loss: 0.1948 \n",
      "Epoch [3/5], Step [500/938], Loss: 0.1961 \n",
      "Epoch [3/5], Step [600/938], Loss: 0.1956 \n",
      "Epoch [3/5], Step [700/938], Loss: 0.1949 \n",
      "Epoch [3/5], Step [800/938], Loss: 0.1936 \n",
      "Epoch [3/5], Step [900/938], Loss: 0.1964 \n",
      "Epoch [4/5], Step [100/938], Loss: 0.1947 \n",
      "Epoch [4/5], Step [200/938], Loss: 0.1946 \n",
      "Epoch [4/5], Step [300/938], Loss: 0.1953 \n",
      "Epoch [4/5], Step [400/938], Loss: 0.1939 \n",
      "Epoch [4/5], Step [500/938], Loss: 0.1914 \n",
      "Epoch [4/5], Step [600/938], Loss: 0.1944 \n",
      "Epoch [4/5], Step [700/938], Loss: 0.1933 \n",
      "Epoch [4/5], Step [800/938], Loss: 0.1927 \n",
      "Epoch [4/5], Step [900/938], Loss: 0.1957 \n",
      "Epoch [5/5], Step [100/938], Loss: 0.1926 \n",
      "Epoch [5/5], Step [200/938], Loss: 0.1950 \n",
      "Epoch [5/5], Step [300/938], Loss: 0.1949 \n",
      "Epoch [5/5], Step [400/938], Loss: 0.1951 \n",
      "Epoch [5/5], Step [500/938], Loss: 0.1941 \n",
      "Epoch [5/5], Step [600/938], Loss: 0.1957 \n",
      "Epoch [5/5], Step [700/938], Loss: 0.1942 \n",
      "Epoch [5/5], Step [800/938], Loss: 0.1931 \n",
      "Epoch [5/5], Step [900/938], Loss: 0.1940 \n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "# Dimension (D) of the representations\n",
    "D = 10\n",
    "lam = 1\n",
    "mu = 0.1\n",
    "nu = 1e-09\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.1307, std=0.3081)])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class CNN_augs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_augs, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(486, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # change output channels to multiples of 2 --> maybe 16, 32??\n",
    "        self.fc3 = nn.Linear(128,D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1,486)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def data_aug(self, img_tensor):\n",
    "        aug = transforms.RandomResizedCrop(20, scale=(0.08,0.1))(img_tensor)\n",
    "        aug = transforms.RandomHorizontalFlip(p=0.5)(aug)\n",
    "        aug = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)(aug)\n",
    "        #aug = transforms.RandomGrayscale(0.2)(aug)\n",
    "        aug = transforms.GaussianBlur(kernel_size=23, sigma=0.5)(aug)\n",
    "        aug = transforms.RandomSolarize(threshold=0.3,p=0.1)(aug)\n",
    "        #aug = transforms.Normalize()(aug)\n",
    "        return aug\n",
    "    def off_diagonal(self, x):\n",
    "        n, m = x.shape\n",
    "        assert n == m\n",
    "        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "        \n",
    "    \n",
    "\n",
    "model_vicreg = CNN_augs().to(device=device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_vicreg.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        # two randomly augmented versions of image\n",
    "        image_i = model_vicreg.data_aug(images)\n",
    "        image_j = model_vicreg.data_aug(images)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        #compute representations\n",
    "        output_i = model_vicreg(image_i)\n",
    "        output_j = model_vicreg(image_j)\n",
    "\n",
    "        #invariance loss\n",
    "        sim_loss = nn.MSELoss()\n",
    "        sim_loss = lam * sim_loss(output_i, output_j)\n",
    "\n",
    "        #variance loss\n",
    "        std_output_i = torch.sqrt(torch.var(output_i, dim=0) + 1e-04)\n",
    "        std_output_j = torch.sqrt(torch.var(output_j, dim = 0) + 1e-04)\n",
    "        std_loss = torch.mean(F.relu(1-std_output_i)) + torch.mean(F.relu(1-std_output_j))\n",
    "\n",
    "        #covariance loss\n",
    "        output_i = output_i - torch.mean(output_i, dim=0)\n",
    "        output_j = output_j - torch.mean(output_j, dim=0)\n",
    "        cov_output_i = (torch.matmul(torch.transpose(output_i, 0, 1), output_i) / (batch_size -1))\n",
    "        cov_output_j = (torch.matmul(torch.transpose(output_j, 0, 1), output_j) / (batch_size -1))\n",
    "        cov_loss = (model_vicreg.off_diagonal(cov_output_i).pow(2).sum() / D) + (model_vicreg.off_diagonal(cov_output_j).pow(2).sum() / D)\n",
    "\n",
    "        # compute loss between two different data augs!\n",
    "        loss = (sim_loss) + (mu * std_loss) + (nu*cov_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream tasks\n",
    "- **Classification**\n",
    "- Regression\n",
    "- Simulate representation of the data\n",
    "- Autoencoder --> anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 0, 0, 0],\n",
       "        [0, 5, 0, 0],\n",
       "        [0, 0, 6, 0],\n",
       "        [0, 0, 0, 7]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[4,0,0,0],[0,5,0,0],[0,0,6,0],[0,0,0,7]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    # checking we have a square matrix where columns and rows are the same\n",
    "    assert n == m\n",
    "    off_diag = x.clone()\n",
    "    off_diag = off_diag.flatten()\n",
    "    # getting all elements except the last corner element\n",
    "    off_diag  = off_diag[:-1]\n",
    "    # reshape the tensor with remaining elements\n",
    "    off_diag = off_diag.view(n-1, n+1)\n",
    "    # the first slice is gathering all rows, the second slice is getting every columns except the first \n",
    "    off_diag = off_diag[:, 1:]\n",
    "    # flattening matrix\n",
    "    off_diag = off_diag.flatten()\n",
    "    return off_diag\n",
    "    #return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off = off_diagonal(a)\n",
    "off.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,4,3])\n",
    "b = torch.pow(a,2).sum()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "print(a)\n",
    "tran = torch.transpose(a, 0, 1)\n",
    "print(tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
