{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2e1ed98b0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True)\n",
    "image, label = train_dataset[11]\n",
    "print(label)\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.1307, std=0.3081)])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(1014, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, 1014)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 12, 28, 28])\n",
      "torch.Size([64, 6, 26, 26])\n",
      "torch.Size([64, 6, 13, 13])\n",
      "torch.Size([64, 256])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "data = torch.rand(N,1,28,28)\n",
    "print(data.shape)\n",
    "conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "max_pool = nn.MaxPool2d(2, 2)\n",
    "x = conv1(data)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = max_pool(x)\n",
    "print(x.shape)\n",
    "x = x.view(-1,1014)\n",
    "fc1 = nn.Linear(1014, 256)\n",
    "x = fc1(x)\n",
    "print(x.shape)\n",
    "fc2 = nn.Linear(256, 128)\n",
    "x = fc2(x)\n",
    "print(x.shape)\n",
    "fc3 = nn.Linear(128,10)\n",
    "x = fc3(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of one MNIST image torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device=device)\n",
    "out = model(train_features)\n",
    "print(f'Output shape of one MNIST image {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/938], Loss: 0.4318 \n",
      "Epoch [1/100], Step [200/938], Loss: 0.0852 \n",
      "Epoch [1/100], Step [300/938], Loss: 0.2245 \n",
      "Epoch [1/100], Step [400/938], Loss: 0.0648 \n",
      "Epoch [1/100], Step [500/938], Loss: 0.0839 \n",
      "Epoch [1/100], Step [600/938], Loss: 0.1503 \n",
      "Epoch [1/100], Step [700/938], Loss: 0.0694 \n",
      "Epoch [1/100], Step [800/938], Loss: 0.0759 \n",
      "Epoch [1/100], Step [900/938], Loss: 0.2175 \n",
      "Epoch [2/100], Step [100/938], Loss: 0.0844 \n",
      "Epoch [2/100], Step [200/938], Loss: 0.0885 \n",
      "Epoch [2/100], Step [300/938], Loss: 0.1121 \n",
      "Epoch [2/100], Step [400/938], Loss: 0.0771 \n",
      "Epoch [2/100], Step [500/938], Loss: 0.0267 \n",
      "Epoch [2/100], Step [600/938], Loss: 0.0899 \n",
      "Epoch [2/100], Step [700/938], Loss: 0.1002 \n",
      "Epoch [2/100], Step [800/938], Loss: 0.0640 \n",
      "Epoch [2/100], Step [900/938], Loss: 0.0185 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cpare/repos/VICReg_project/VICReg.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m n_total_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:134\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    131\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:98\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:141\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    139\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m'\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m    140\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[0;32m--> 141\u001b[0m     np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/PIL/Image.py:537\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exif \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    539\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    540\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImage categories are deprecated and will be removed in Pillow 10 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(2023-01-02). Use is_animated instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    542\u001b[0m             \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    544\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = CNN().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
