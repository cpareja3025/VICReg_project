{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2cc6e8a00>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True)\n",
    "image, label = train_dataset[11]\n",
    "print(label)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "a = transforms.Resize(252)(image)\n",
    "plt.imshow(a, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.1307, std=0.3081)])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img_tensor):\n",
    "    aug = transforms.RandomResizedCrop(224, scale=(0.08,0.1))(img_tensor)\n",
    "    aug = transforms.RandomHorizontalFlip(p=0.5)(aug)\n",
    "    aug = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)(aug)\n",
    "    #aug = transforms.RandomGrayscale(0.2)(aug)\n",
    "    aug = transforms.GaussianBlur(kernel_size=23, sigma=0.5)(aug)\n",
    "    aug = transforms.RandomSolarize(threshold=0.3,p=0.1)(aug)\n",
    "    #aug = transforms.Normalize()(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(1014, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, 1014)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 224, 224])\n",
      "torch.Size([64, 12, 224, 224])\n",
      "torch.Size([64, 6, 222, 222])\n",
      "torch.Size([64, 6, 111, 111])\n",
      "torch.Size([64, 256])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "data = torch.rand(N,1,28,28)\n",
    "data = data_aug(data)\n",
    "print(data.shape)\n",
    "conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "max_pool = nn.MaxPool2d(2, 2)\n",
    "x = conv1(data)\n",
    "print(x.shape)\n",
    "x = conv2(x)\n",
    "print(x.shape)\n",
    "x = max_pool(x)\n",
    "print(x.shape)\n",
    "x = x.view(-1,73926)\n",
    "fc1 = nn.Linear(73926, 256)\n",
    "x = fc1(x)\n",
    "print(x.shape)\n",
    "fc2 = nn.Linear(256, 128)\n",
    "x = fc2(x)\n",
    "print(x.shape)\n",
    "fc3 = nn.Linear(128,10)\n",
    "x = fc3(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of one MNIST image torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device=device)\n",
    "out = model(train_features)\n",
    "print(f'Output shape of one MNIST image {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/469], Loss: 0.3038 \n",
      "Epoch [1/20], Step [200/469], Loss: 0.2584 \n",
      "Epoch [1/20], Step [300/469], Loss: 0.1767 \n",
      "Epoch [1/20], Step [400/469], Loss: 0.1617 \n",
      "Epoch [2/20], Step [100/469], Loss: 0.2216 \n",
      "Epoch [2/20], Step [200/469], Loss: 0.1035 \n",
      "Epoch [2/20], Step [300/469], Loss: 0.0845 \n",
      "Epoch [2/20], Step [400/469], Loss: 0.1242 \n",
      "Epoch [3/20], Step [100/469], Loss: 0.0643 \n",
      "Epoch [3/20], Step [200/469], Loss: 0.0917 \n",
      "Epoch [3/20], Step [300/469], Loss: 0.0717 \n",
      "Epoch [3/20], Step [400/469], Loss: 0.0790 \n",
      "Epoch [4/20], Step [100/469], Loss: 0.2221 \n",
      "Epoch [4/20], Step [200/469], Loss: 0.0387 \n",
      "Epoch [4/20], Step [300/469], Loss: 0.0663 \n",
      "Epoch [4/20], Step [400/469], Loss: 0.0246 \n",
      "Epoch [5/20], Step [100/469], Loss: 0.0178 \n",
      "Epoch [5/20], Step [200/469], Loss: 0.0096 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cpare/repos/VICReg_project/VICReg.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model = CNN().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        # images = data_aug(images)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Torch Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_augs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_augs, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(12, 6, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(73926, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # change output channels to multiples of 2 --> maybe 16, 32??\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1,73926)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_augs                                 [128, 10]                 --\n",
       "├─Conv2d: 1-1                            [128, 12, 224, 224]       24\n",
       "├─Conv2d: 1-2                            [128, 6, 222, 222]        654\n",
       "├─MaxPool2d: 1-3                         [128, 6, 111, 111]        --\n",
       "├─Linear: 1-4                            [128, 256]                18,925,312\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 10]                 1,290\n",
       "==========================================================================================\n",
       "Total params: 18,960,176\n",
       "Trainable params: 18,960,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.71\n",
       "==========================================================================================\n",
       "Input size (MB): 25.69\n",
       "Forward/backward pass size (MB): 919.77\n",
       "Params size (MB): 75.84\n",
       "Estimated Total Size (MB): 1021.30\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_augs().to(device=device)\n",
    "summary(model, input_size=[(batch_size, 1, 224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cpare/repos/VICReg_project/VICReg.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m images \u001b[39m=\u001b[39m data_aug(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/cpare/repos/VICReg_project/VICReg.ipynb Cell 13\u001b[0m in \u001b[0;36mCNN_augs.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cpare/repos/VICReg_project/VICReg.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m73926\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model = CNN_augs().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        images = data_aug(images)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        # compute loss between two different data augs!\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(N,1,28,28)\n",
    "image_sample = train_features\n",
    "aug = transforms.RandomResizedCrop(224, scale=(0.08,0.1))(train_features)\n",
    "print(aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img_tensor):\n",
    "    #change to (20, 20)\n",
    "    aug = transforms.RandomResizedCrop(224, scale=(0.08,0.1))(img_tensor)\n",
    "    aug = transforms.RandomHorizontalFlip(p=0.5)(aug)\n",
    "    aug = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)(aug)\n",
    "    #aug = transforms.RandomGrayscale(0.2)(aug)\n",
    "    aug = transforms.GaussianBlur(kernel_size=23, sigma=0.5)(aug)\n",
    "    aug = transforms.RandomSolarize(threshold=0.3,p=0.1)(aug)\n",
    "    #aug = transforms.Normalize()(aug)\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "transformed_image = data_aug(image_sample)\n",
    "print(transformed_image.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream tasks\n",
    "- **Classification**\n",
    "- Regression\n",
    "- Simulate representation of the data\n",
    "- Autoencoder --> anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
